{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.13.0\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting argon2-cffi==21.1.0\n",
      "  Downloading argon2_cffi-21.1.0-cp35-abi3-macosx_10_14_x86_64.whl (38 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting attrs==21.2.0\n",
      "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 6.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: backcall==0.2.0 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 5)) (0.2.0)\n",
      "Collecting bleach==4.1.0\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools==4.2.2\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting certifi==2021.5.30\n",
      "  Using cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting cffi==1.14.6\n",
      "  Downloading cffi-1.14.6-cp38-cp38-macosx_10_9_x86_64.whl (176 kB)\n",
      "\u001b[K     |████████████████████████████████| 176 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer==2.0.4\n",
      "  Using cached charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting clang==5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: colorama==0.4.4 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 12)) (0.4.4)\n",
      "Requirement already satisfied: cycler==0.10.0 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 13)) (0.10.0)\n",
      "Collecting debugpy==1.4.1\n",
      "  Downloading debugpy-1.4.1-cp38-cp38-macosx_10_14_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting decorator==5.0.9\n",
      "  Downloading decorator-5.0.9-py3-none-any.whl (8.9 kB)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 16)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints==0.3 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 17)) (0.3)\n",
      "Requirement already satisfied: filelock==3.0.12 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 18)) (3.0.12)\n",
      "Collecting flatbuffers==1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-auth==1.35.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 53.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib==0.4.6\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-pasta==0.2.0\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio==1.40.0\n",
      "  Downloading grpcio-1.40.0-cp38-cp38-macosx_10_10_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py==3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 34.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna==3.2\n",
      "  Using cached idna-3.2-py3-none-any.whl (59 kB)\n",
      "Collecting ipykernel==6.3.1\n",
      "  Downloading ipykernel-6.3.1-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipython==7.27.0\n",
      "  Downloading ipython-7.27.0-py3-none-any.whl (787 kB)\n",
      "\u001b[K     |████████████████████████████████| 787 kB 26.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 29)) (0.2.0)\n",
      "Collecting ipywidgets==7.6.4\n",
      "  Downloading ipywidgets-7.6.4-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 52.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jedi==0.18.0\n",
      "  Downloading jedi-0.18.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Jinja2==3.0.1\n",
      "  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 48.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib==1.0.1 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 33)) (1.0.1)\n",
      "Requirement already satisfied: jsonschema==3.2.0 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 34)) (3.2.0)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 35)) (1.0.0)\n",
      "Collecting jupyter-client==7.0.2\n",
      "  Downloading jupyter_client-7.0.2-py3-none-any.whl (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jupyter-console==6.4.0 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 37)) (6.4.0)\n",
      "Requirement already satisfied: jupyter-core==4.7.1 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 38)) (4.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 39)) (0.1.2)\n",
      "Collecting jupyterlab-widgets==1.0.1\n",
      "  Downloading jupyterlab_widgets-1.0.1-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras==2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 38.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Keras-Preprocessing==1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver==1.3.2\n",
      "  Using cached kiwisolver-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\n",
      "Requirement already satisfied: lxml==4.6.3 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 44)) (4.6.3)\n",
      "Collecting Markdown==3.3.4\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe==2.0.1\n",
      "  Downloading MarkupSafe-2.0.1-cp38-cp38-macosx_10_9_x86_64.whl (13 kB)\n",
      "Collecting matplotlib==3.4.3\n",
      "  Using cached matplotlib-3.4.3-cp38-cp38-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "Collecting matplotlib-inline==0.1.3\n",
      "  Downloading matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: mistune==0.8.4 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 49)) (0.8.4)\n",
      "Collecting nbclient==0.5.4\n",
      "  Downloading nbclient-0.5.4-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nbconvert==6.1.0\n",
      "  Downloading nbconvert-6.1.0-py3-none-any.whl (551 kB)\n",
      "\u001b[K     |████████████████████████████████| 551 kB 53.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbformat==5.1.3 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 52)) (5.1.3)\n",
      "Requirement already satisfied: nest-asyncio==1.5.1 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 53)) (1.5.1)\n",
      "Collecting notebook==6.4.3\n",
      "  Downloading notebook-6.4.3-py3-none-any.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 42.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.6 MB 52.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib==3.1.1\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 35.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum==3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.3 MB/s eta 0:00:011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting packaging==21.0\n",
      "  Using cached packaging-21.0-py3-none-any.whl (40 kB)\n",
      "Collecting pandas==1.3.2\n",
      "  Downloading pandas-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (11.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.4 MB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas-datareader==0.10.0\n",
      "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 32.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandocfilters==1.4.3 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 61)) (1.4.3)\n",
      "Collecting parso==0.8.2\n",
      "  Downloading parso-0.8.2-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 8.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 63)) (0.7.5)\n",
      "Collecting Pillow==8.3.2\n",
      "  Downloading Pillow-8.3.2-cp38-cp38-macosx_10_10_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ply==3.11 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 65)) (3.11)\n",
      "Collecting prometheus-client==0.11.0\n",
      "  Downloading prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting prompt-toolkit==3.0.20\n",
      "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
      "\u001b[K     |████████████████████████████████| 370 kB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf==3.17.3\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1==0.4.8\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules==0.2.8\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pycparser==2.20 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 71)) (2.20)\n",
      "Collecting Pygments==2.10.0\n",
      "  Using cached Pygments-2.10.0-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 73)) (2.4.7)\n",
      "Collecting pyrsistent==0.18.0\n",
      "  Downloading pyrsistent-0.18.0-cp38-cp38-macosx_10_9_x86_64.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 7.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: PySocks==1.7.1 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 75)) (1.7.1)\n",
      "Collecting python-dateutil==2.8.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: pytz==2021.1 in /Users/vinayreddy/opt/anaconda3/lib/python3.8/site-packages (from -r ../requirements.txt (line 77)) (2021.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==301\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pywin32==301\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stock Market Predictions with a Long Short-Term Memory Neural Network\n",
    "\n",
    "Say you're planning to invest in the stock market, so you want to model fluctuations in price by looking at the history of a sequence of prices to accurately predict what future prices will be. When analyzing a sequence of data which were observed in some constant increment of time, and each observation is directly dependent on one or more previous observations (a stock price tomorrow directly depends on its price today), you need a time series model. In this workshop, we'll start by investigating two well-known models, then compare their prediction accuracy to an LSTM nueral network.\n",
    "\n",
    "Adapted from: https://www.datacamp.com/community/tutorials/lstm-python-stock-market\n",
    "\n",
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_datareader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0971cc244890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_datareader'"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The tutorial that this workshop was adapted from outlines two sources of data for use in the remainder of the workshop. For simplicity, we'll stick with the Kaggle data set that was provided. Feel free to tinker with analyzing different stock symbols (i.e., data for different companies' stock prices), although I can't guarantee that everything will work as I've only tested the code with the Kaggle data for Hewlett-Packard (HP). Generally speaking, stock prices can be measured with the following metrics:\n",
    "- Open: Opening stock price of a time period\n",
    "- Close: Closing stock price of a time period\n",
    "- High: Highest stock price of a time period\n",
    "- Low: Lowest stock price of a time period\n",
    "\n",
    "Note that these metrics can be analyzed for various time intervals (e.g., daily, hourly, 15 minutes, 5 minutes, etc.), but in this workshop, we'll focus on daily prices over the course of multiple years. In theory, you could build a model on any interval of time you have data for, but exploring the benefits and drawbacks of such variations are outside of the scope of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_source = 'kaggle' # alphavantage or kaggle\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    api_key = 'WFYTJ3SYJWWDIHQ8'\n",
    "\n",
    "    # American Airlines stock market prices\n",
    "    ticker = 'AAL'\n",
    "\n",
    "    # JSON file with all the stock market data for AAL from the last 20 years\n",
    "    url_string = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&outputsize=full&apikey={api_key}'\n",
    "\n",
    "    # Save data to this file\n",
    "    file_to_save = f'../data/raw/stock_market_data-{ticker}.csv'\n",
    "\n",
    "    # If you haven't already saved data,\n",
    "    # Go ahead and grab the data from the url\n",
    "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            # extract stock market data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date', 'Low', 'High', 'Close', 'Open'])\n",
    "            for k,v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(), float(v['3. low']), float(v['2. high']), float(v['4. close']), float(v['1. open'])]\n",
    "                df.loc[-1,:] = data_row\n",
    "                df.index = df.index + 1\n",
    "        print(f'Data saved to : {file_to_save}')\n",
    "        df.to_csv(file_to_save)\n",
    "\n",
    "    # If the data is already there, just load it from the CSV\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "\n",
    "else:\n",
    "    # You will be using HP's data. Feel free to experiment with other data.\n",
    "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "    df = pd.read_csv(os.path.join('../data/external/Stocks', 'hpq.us.txt'), delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close'])\n",
    "    print('Loaded data from the Kaggle repository')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and Check Data\n",
    "\n",
    "Note that it is extremely important for time series data to be ordered by time, otherwise you would be training your model on some arbitrary sequence of observations which may be detrimental to its efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "plt.plot(range(df.shape[0]), (df['Low'] + df['High']) / 2.0)\n",
    "plt.xticks(range(0, df.shape[0], 500), df['Date'].loc[::500], rotation=45)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "As per usual, you should split your data into training and testing sets, so your model is validated upon its predicitons for observations it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:, 'High'].to_numpy()\n",
    "low_prices = df.loc[:, 'Low'].to_numpy()\n",
    "mid_prices = (high_prices + low_prices) / 2.0\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data\n",
    "\n",
    "Before training a model, you must normalize the data. Since different time periods of data have different value ranges, we normalize the data by \"binning\" the full time series into windows of some specified size (in this case it is 2500). We then smooth **only** the training data, using the exponential moving average, to reduce the amount of noise our models encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1, 1)\n",
    "test_data = test_data.reshape(-1, 1)\n",
    "\n",
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0, 10000, smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di + smoothing_window_size, :])\n",
    "    train_data[di:di + smoothing_window_size, :] = scaler.transform(train_data[di:di + smoothing_window_size, :])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di + smoothing_window_size:, :])\n",
    "train_data[di + smoothing_window_size:, :] = scaler.transform(train_data[di + smoothing_window_size:, :])\n",
    "\n",
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)\n",
    "\n",
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma * train_data[ti] + (1 - gamma) * EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Step Ahead Prediction via Averaging\n",
    "\n",
    "We will compare different methods of modeling the stock price time series we have based on Mean Squared Error (MSE), which is calculated by averaging the squared error of each prediction we generate over all observations.\n",
    "\n",
    "### Standard Average\n",
    "$$x_{t+1}=\\frac{1}{N}\\sum_{i=t-N}^t x_i$$\n",
    "In this case, we're saying the prediction at time $t+1$ is the average of the stock prices observed within a window of time $t-N$ to time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size, N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx, 'Date']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx - window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1] - train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print(f'MSE error for standard averaging: {0.5 * np.mean(mse_errors):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='True')\n",
    "plt.plot(range(window_size, N), std_avg_predictions, color='orange', label='Prediction')\n",
    "# plt.xticks(range(0, df.shape[0], 50), df['Date'].loc[::50], rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model's predictions follow the actual behavior of the stock prices fairly accurately, although it seems to lag behind the actual price movement in the market by a few days. It seems as though this model is relatively useful for making short-term price predictions (i.e., a day or two ahead), but we will continue to investigate further.\n",
    "\n",
    "### Exponential Moving Average\n",
    "$$x_{t+1}=EMA_t=\\gamma\\times EMA_{t-1}+(1-\\gamma)x_t$$\n",
    "Here, $EMA_0=0$ and $EMA$ is the exponential moving average value you maintain over time. When predicting price for time $t+1$, $\\gamma$ dictates how the immediately preceding observation (time $t$) is weighted against the prior moving average for time $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1, N):\n",
    "\n",
    "    running_mean = running_mean * decay + (1.0 - decay)*train_data[pred_idx - 1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1] - train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print(f'MSE error for EMA averaging: {0.5 * np.mean(mse_errors):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='True')\n",
    "plt.plot(range(0, N), run_avg_predictions, color='orange', label='Prediction')\n",
    "#plt.xticks(range(0, df.shape[0], 50), df['Date'].loc[::50], rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that the line of predictions nearly perfectly mirrors the actual stock price movement, but is it really that useful? In practical applications, you would ideally like to be able to make predictions for times $t+1$, $t+2$, etc. For the two models we just explored, however, you're only ever able to make a single prediction for the subsequent period of time (time $t+1$). What if you instead wanted to make a prediction 30 days in advance? For this purpose, we will explore the use of long short-term memory nueral networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
